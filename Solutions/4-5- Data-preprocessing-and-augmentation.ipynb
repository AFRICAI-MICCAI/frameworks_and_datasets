{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pil(image):\n",
    "    from PIL import Image\n",
    "    from IPython.display import display\n",
    "    data = image.numpy().squeeze().T\n",
    "    data = data.astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    w, h = image.size\n",
    "    display(image)\n",
    "    print()  # in case multiple images are being displayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tio.ScalarImage(os.path.join(data_dir,'vhp', 'a_vm1125.png'))\n",
    "img.affine[0, 0] = img.affine[1, 1] = 0.33  # according to https://www.nlm.nih.gov/databases/download/vhp.html\n",
    "print(img)\n",
    "img.as_pil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_ap = tio.RandomFlip(axes=['anteroposterior'], flip_probability=1)\n",
    "flip_lr = tio.RandomFlip(axes=['lateral'], flip_probability=0.5)\n",
    "crop = tio.CropOrPad((800, 800, 1))\n",
    "resample = tio.Resample(0.75)\n",
    "elastic = tio.RandomElasticDeformation(max_displacement=50)\n",
    "affine = tio.RandomAffine()\n",
    "spatial = tio.OneOf({\n",
    "    elastic: 0.6,\n",
    "    affine: 0.4,\n",
    "})\n",
    "blur = tio.RandomBlur(p=0.75)\n",
    "noise = tio.RandomNoise(mean=128, std=10)\n",
    "rescale = tio.RescaleIntensity((0, 255))\n",
    "noise_rescale = tio.Compose([noise, rescale])\n",
    "rgb2gray = tio.Lambda(lambda tensor: torch.mean(tensor, 0, keepdim=True))\n",
    "\n",
    "transforms = [\n",
    "    flip_ap,\n",
    "    flip_lr,\n",
    "    crop,\n",
    "    resample,\n",
    "    spatial,\n",
    "    blur,\n",
    "    noise_rescale,\n",
    "    rgb2gray,\n",
    "]\n",
    "\n",
    "transform = tio.Compose(transforms)\n",
    "\n",
    "[to_pil(transform(img)) for _ in range(10)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to bonus exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "def apply_transforms(image, transforms, seed=42, show=False, exclude=None):\n",
    "    torch.manual_seed(seed)\n",
    "    results = []\n",
    "    transformed = image\n",
    "    tic_all = time.time()\n",
    "    for transform in transforms:\n",
    "        tic = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            result = transform(transformed)\n",
    "            # We want the transform to be applied even if it is excluded to ensure reproducibility\n",
    "            if exclude is None or transform.name not in exclude:\n",
    "                transformed = result\n",
    "        millis = int((time.time() - tic) * 1000)\n",
    "        print(f'{transform.name:12}{millis:>5} ms')\n",
    "        results.append(transformed)\n",
    "    millis = int((time.time() - tic_all) * 1000)\n",
    "    print(f'{\"TOTAL\":12}{millis:>5} ms')\n",
    "    if show:\n",
    "        [to_pil(im) for im in results];\n",
    "\n",
    "print('All transforms:')\n",
    "apply_transforms(img, transforms)\n",
    "\n",
    "print('\\nWithout cropping:')\n",
    "apply_transforms(img, transforms, exclude=['CropOrPad'])\n",
    "\n",
    "print('\\nWithout resampling:')\n",
    "apply_transforms(img, transforms, exclude=['Resample'])\n",
    "\n",
    "print('\\nWithout cropping and resampling:')\n",
    "apply_transforms(img, transforms, exclude=['Resample', 'CropOrPad'])\n",
    "\n",
    "# Cropping and resampling makes our code one order of magnitude faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
